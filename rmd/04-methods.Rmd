# Statistical methods

## Variance decomposition of TX1day $T'$

The aim of this thesis is to gain further understanding in the influence of the three $T'$-generating mechanisms on TX1day events, particularly relating to the variability in yearly maxima observations. The ideal output of a statistical analysis would then be quantifying the influence of each contributor on a second-moment estimate of yearly maxima TX1day magnitude at each location. 

Ranking the influence a certain predictor has on a response relative to other predictors is a common problem in applied statistics. Although $R^2$ is often reported to provide comparisons of model fit and thus measure the value of certain predictors in explaining the variance of the response, it does not take into account interactions between predictors. Instead, in the context of linear regression models, \cite{lindeman_introduction_1980} - referred to as LMG - propose to quantify predictor importance by computing the average sequential sums-of-squares over all predictor orderings. Averaging over all orderings allows this measure to capture both direct effects and secondary effects of a predictor. \cite{feldman_relative_2005} extended this approach with Proportional Marginal Variance Decomposition (PMVD) that instead considers a weighted average with weights chosen such as to enforce the exclusion criteria - a predictor with zero coefficient is allocated zero importance \citep{gromping_estimators_2007}. \cite{breiman_random_2001}, and extensions thereof, instead propose to measure importance of a predictor by assessing the model's loss of accuracy due to random permutations of the predictor values. 

These methods have however been shown to be susceptible to data with highly correlated predictors, as is often the case in observational settings lacking causal assumptions. Furthermore, with many predictors, LMG and PMVD become computationally intensive, as they require computation of $R^2$ values over all possible orderings. Permutation-based methods scale linearly with the number of predictors but may require running many permutations of each column to output estimates with low variance.

[Chapter 3][Data] has indeed demonstrated strong correlations between contributors that are due to inherent physical constraints. In addition, by assuming residuals to be approximately zero, the response and predictors are linearly dependent. This then limits the applicability of standard regression methods and the associated measures of predictor importance that assume them uncorrelated \citep{hooker_unrestricted_2021}. Addressing the constraint that the response is the sum of the three contributors, for example by transforming the contributors to a simplex space \citep{greenacre_compositional_2021}, is also problematic as the data does not follow the positivity constraint: contributions to $T'$ may be negative. Pre-processing the data to enforce positivity leads to poor representation of the domain and hinders interpretation. Furthermore, more robust scale measures - such as truncated variance - would not be appropriate as information of tail events that are often associated with the largest human impact needs to be preserved.

Then, to extend the first-moment characterization of TX1day events by \cite{rothlisberger_quantifying_2023}, we propose a variance decomposition of $T'$ that quantifies the contributions of physical processes to variability. Recall that computation of \@ref(eq:tdecomp) gave rise to residuals that may be assumed zero. Then the final $T'$ of the event is related linearly to the $T'$ generated by the three processes with a bias from the seasonality term, which is is generally small. Then, for $n$ independent events achieving temperature anomalies $\{ T' \}_{1:n}$, data is described by ${T'}_i \approx \text{adv}_i +\text{adiab}_i +\text{diab}_i$ and therefore:

\begin{equation}
   V(\{ T' \}_{1:n}) \approx \sum_{c \in \{\text{adv},\text{adiab},\text{diab}\}} V(\{ c \}_{1:n} ) + \sum_{c, d \in \{\text{adv},\text{adiab},\text{diab}\} , c\neq d} \text{Cov}(\{ c \}_{1:n} ,\{ d \}_{1:n})
(\#eq:vardecomp)
\end{equation}

For further support, a brief background on variance and associated estimates is presented in Appendix B. 

Applying \@ref(eq:vardecomp) to every location on Earth yields a direct and systematic approach to the analysis of global patterns of hot extreme variability, while the presence of complex dependencies between physical processes is explored by the contributions from covariance terms. The linearity of the covariance operator and the fact that it is un-normalized, in comparison to correlation for instance, allows for an intuitive comparison of second-moment contributions. We argue that considering the variance of the contributors without adjusting for the influence of other contributors due to existing correlation is more informative. For example, a large variation in adiabatic $T'$ contribution is important to report, regardless of the additional influence of advective and diabatic processes; if the sum of adiabatic and advective $T'$ has small variance, it will be reflected in a negative covariance between the contributors. The limitation of using an un-normalized quantity is the fact that a large mean magnitude will in most cases imply a larger variance. Therefore, meaningful interpretation of second-moment estimates will also require consideration of first-moment estimates.

To summarize the spatial structure of TX1day $T'$ variance and its decomposition, variance terms will be ranked with respect to their magnitude according to the following definition (applied to both mean and variance decomposition data): a contributor is dominant if it is at least twice as large as the second largest contributor; two contributors are dominant if they are both at least twice as large as the remaining contributor; else, no contributor is dominant. As both mean and variance decompositions are additive, this importance definition allows for a intuitive comparison. Covariance terms are omitted in these comparisons as they are observed to be generally more negative than the variance terms, except in few exceptions.

## Principle Component Analysis of contributors

As discussed, strong inter-contributor correlations limit the interpretability of the variance decomposition, but are important in understanding the physical mechanisms associated with hot extreme development and their constraints. To characterize the dependence structure between the contributors, Principal Component Analysis (PCA) is employed to assess the degrees of freedom of the heat-generating systems and provide an alternative importance definition to rank the contributors at each location. 

PCA - known as Empirical Orthogonal Functions in climate sciences - is a linear transformation introduced by \cite{pearson_liii_1901} and first applied in climate sciences for weather prediction by \cite{lorenz_empirical_1956}. It is commonly used as a dimension reduction method - to obtain a low-dimensional representation of high-dimensional inputs - and in identifying and quantifying modes of variability such as the North-Atlantic Oscillation and the ENSO. The PCA problem consists in finding a projection $\mathbf{f}(\mathbf{x})$ that yields lower dimensional and orthogonal representations of the data that retain the most information.

Given $n$ mean-centered samples $\mathbf{X} = (\mathbf{x}_1, ..., \mathbf{x}_n) \in \mathbb{R}^{n \times p}$ of dimension $p$, the solution to obtain PCA transformation $\mathbf{W} = (w)_{ij} \in \mathbb{R}^{p \times l}$ for $1 \leq l \leq p$ and $l-$dimensional representations called PC scores $\mathbf{z}_1, ..., \mathbf{z}_{n} \in \mathbb{R}^l$ is given by:
\begin{equation}
(\mathbf{W}, \mathbf{z}_1, ..., \mathbf{z}_{n}) = \argmin_{\mathbf{W}^{\intercal}\mathbf{W}=\mathbf{I}_l,\mathbf{z}} \, \sum_{i=1}^n \left|\left| \mathbf{z}_i  \mathbf{W}^{\intercal}- \mathbf{x}_i \right|\right|^2_2
(\#eq:pcatrans)
\end{equation}

with $f(\mathbf{x}_i) = \mathbf{z}_i = \mathbf{x}_i \mathbf{W}$. The unit column vectors $\mathbf{w}_{\bullet 1}, ...,  \mathbf{w}_{\bullet l}$ are related to the empirical covariance matrix $\boldsymbol{\Sigma}$ of $\mathbf{X}$ as they are its eigenvectors corresponding to its eigenvalues $\lambda_{(1)}, ..., \lambda_{(p)}$ ranked in decreasing magnitude: 
\begin{equation}
\boldsymbol{\Sigma} = \sum_{i=1}^p \lambda_{(i)} \mathbf{w}_{\bullet i} \mathbf{w}_{\bullet i}^{\intercal} 
(\#eq:pcacov)
\end{equation}

The first PC (PC1) then points in the direction with the largest variance, PC2 points in the direction orthogonal to PC1 with the largest residual variance, etc. The proportion of variance explained by a single PC can be computed using the eigenvalues: 
\begin{equation}
\text{\% explained variance }(\text{PC}i) = \frac{\lambda_{(i)}}{\lambda_{(1)}+...+\lambda_{(p)}}
(\#eq:pcaexplvar)
\end{equation}

Untruncated PCA is then applied to the advective, adiabatic and diabatic $T'$ contributions of the 41 events at each location. To present an alternative measure of importance than the variance decomposition, each contributor is centered and scaled independently. This focuses the analysis on the contributor dependencies since it compares the variability structure without the influence of absolute magnitudes. The components are computed exactly using Singular Value Decomposition as proposed by \cite{halko_finding_2011} and the analysis will consider the proportion of explained variance for each PC and relate the resulting directions of the PCs to the contributors. 

Additionally, at each location we have three PC directions - i.e. the unit column vectors $\mathbf{w}_{\bullet 1}, \mathbf{w}_{\bullet 2},  \mathbf{w}_{\bullet 3}$ - that point in the orthogonal directions of largest variability. To represent these new axes at every location, we partition them in 7 categories, similarly to the dominance definition introduced in [4.1][Variance decomposition of TX1day $T'$]. Since the directions are unit-vectors in 3D space, the vector created by applying element-wise absolute-value lies on the positive octant of the surface of a unit sphere. This surface is partitioned into three corner regions (each contributor), the center (combination of all contributors) and three remaining surface divided evenly (pair-combinations), according to figure \@ref(fig:pcascores).

```{r pcascores, fig.cap="Illustration of the partitioning of surface of PC absolute-value vectors into 7 categories: advective (A), adiabatic (B), diabatic (C), advective-adiabatic (D), adiabatic-diabatic (E), diabatic-advective (F) and all (D). The construction points all lie on planes parallel to the axes at values $\\sqrt{2}/2$, $0.5$ and $\\sqrt{3}/2$ that correspond to 30, 45 and 60$^\\circ$ angles between the axial planes.", echo=FALSE, out.width="80%", fig.pos = 'h', fig.align='center'}
knitr::include_graphics("images/pca_partition.png")
```

Finally, PCA is preferred over machine-learning approaches because of the small sample sizes at each location, it is variance-based and therefore can be related to previous work, and has an intuitive interpretation.

## Forecasting TX1day contributor trajectories

Considering final $T'$ is useful for quantifying the cumulative contributions of physical processes to hot extreme events, as well as investigate the overall dependence patterns between them. To extend this analysis and provide a more precise assessment of the behavior of physical processes in the development of hot extreme events, we consider trajectory information. The added temporal structure allows the investigation of the underlying dynamics. For example, rapid $T'$ growth of a certain physical process in the final day before an extreme event can be distinguished from a gradual gain that was previously reported as a single cumulative total. Understanding the timescales of advective, adiabatic and diabatic $T'$ growth are particularly important for the development of warning systems in face of increasingly dangerous hot extreme events. 

Trajectory-based analyses in hot extreme research are often carried out case-by-case. We propose leveraging the flexibility of Deep-Learning (DL) models to investigate the predictability of yearly maxima TX1day $T'$ decomposition trajectories on a global scale. In this setting, a model is trained to forecast advective, adiabatic and diabatic $T$ in the final period of trajectory given their past. A model that performs adequately can then be assumed to have learned meaningful patterns in the data that reflect the true dynamics of the system. Then, the model can be analysed to determine the contributors and time-windows that most affect the prediction, thus unraveling temporal interactions of the physical mechanisms in the development of hot extreme events. Before introducing the model employed in this thesis, we give a brief overview of machine learning. 

Machine learning and in particular DL architectures have recently received a lot of attention in atmospheric and climate sciences due to their performance in modelling complex high-dimensional systems and their computational efficiency in comparison to numerical weather prediction. DL models have largely been used to develop weather forecasting models \citep[see for example][]{srivastava_weather_2022} and to learn parameterizations from simulated and observational datasets \citep{barahona_deep_2023}. We first provide a brief introduction to a DL architecture that has been used extensively in timeseries applications. 

Supervised DL involves the parameterization of arbitrarily complex functions - called networks - and estimation of these parameters by propagating input data through the network and evaluating the outputs' accuracy against the given truth - called the loss. By using a differentiable loss and network, the parameters can be updated. With sufficient data and learning iterations, the model parameters will be progressively updated to improve accuracy. 

The modelling and processing of sequential data requires specific architectures such as Recurrent Neural Networks (RNNs) that can leverage sequential dependencies to learn representations of the underlying dynamical system. At a high level, RNN function by iteratively processing each time-step to update a set of hidden weights that retain memory - a memory cell. However, standard implementations have been shown to suffer from vanishing gradients - when parameter updates approach zero due to decaying gradient magnitudes, thus impeding learning - in applications with long-term temporal dependencies \citep{noh_analysis_2021}. 

Long Short-Term Memory (LSTM) networks \citep{hochreiter_long_1997} were the first extension of RNNs introduced to solve vanishing gradients. They contain a memory cell that is modulated by other units - called gates - to learn to determined when to 'forget' information. More formally, an LSTM memory cell is composed of hidden states $H$, cell states $C$ and four gates: the input, forget, cell and output gates. Given a sequence of data, a memory cell recurrently propagates a data sequence one timestep at a time to update the gate, hidden and cell parameters. Given some initial hidden $H^{(t-1)}$ and cell states $C^{(t-1)}$, an input $x^{(t)}$ at time-step $t$ is processed according to:
\begin{equation}
  \begin{aligned}
  \text{Gating:} \quad & \\
    i^{(t)} &= \sigma \left( W_{ii} x^{(t)} + b_{ii} + W_{hi} H^{(t-1)} + b_{hi}  \right) \\
    f^{(t)} &= \sigma \left( W_{if} x^{(t)} + b_{if} + W_{hf} H^{(t-1)} + b_{hf}  \right) \\
    g^{(t)} &= \tanh \left( W_{ig} x^{(t)} + b_{ig} + W_{hg} H^{(t-1)} + b_{hg}  \right) \\
    o^{(t)} &= \sigma \left( W_{io} x^{(t)} + b_{io} + W_{ho} H^{(t-1)} + b_{ho}  \right)
  \end{aligned}
(\#eq:lstm1)
\end{equation}

\begin{equation}
  \begin{aligned}
  \text{Memory update:} \quad & \\
    C^{(t)} &= f^{(t)} \ast C^{(t-1)} + i^{(t)} \ast g^{(t)} \\
    H^{(t)} &= o^{(t)} \ast \tanh \left( C^{(t)}\right)
  \end{aligned}
(\#eq:lstm2)
\end{equation}

where $\sigma$ and $\tanh$ are the sigma and tanh activation functions; $\ast$ represents element-wise multiplication; $W_{ij}$ and $b_{ij}$ are matrices of learned weight and bias parameters for the input ($j=i$) and the hidden states ($j=h$); $i^{(t)},f^{(t)},g^{(t)}$ and $o^{(t)}$ are the forget, input and output gates at time $t$, respectively. A schematic representation can be found in Figure \@ref(fig:lstm). 

```{r lstm, fig.cap="(a) Representation of an LSTM memory cell (in orange) that, given intitial hidden states $H^{(t-1)}$, cell states $C^{(t-1)}$ and input $x^{(t)}$, outputs the updated hidden $H^{(t)}$ and cell $C^{(t)}$ states. Purple shapes represents operations: $\\ast$ matrix multiplication, $+$ element-wise addition, tanh the tanh activation function, and the Forget, Input and Output gates are as define in equations 4.5. (b) The same LSTM cell but unfolded. For an input of initial hidden states $H^{(0)}$, cell states $C^{(0)}$ and data sequence $x^{(1:T)}$, it outputs final hidden states $H^{(T)}$ that can then be further processed (e.g. by a fully connected layer).", echo=FALSE, out.width="80%", fig.pos = 'h', fig.align='center'}
knitr::include_graphics("images/lstm_fig.png")
```

LSTM layers, composed of a number of memory cells, may used in different ways depending on the data structure and task at hand. In timeseries forecasting applications, most models use the final hidden states $H^{(T)}$ of the last LSTM layer as lower-dimensional feature representations of every input that are then further processed by one or more fully-connected NN layers. There is a rich and evolving literature on LSTM-based architectures and LSTM variants and thus refer the reader to comprehensive review papers such as \cite{hu_time_2020} and \cite{han_review_2021}.

Here, a global LSTM-based DL model is trained to forecast the final 8 timesteps (1 day) of each contributor of the TX1day event decomposition trajectories given their history. The model is composed of a single LSTM layer and a linear output layer to process the final hidden states $H^{(T)}$ into 24 outputs (3 variables x 8 timesteps). The rich diversity of timeseries behaviors and patterns as well as the occurrence of anomalous large negative and positive values motivates careful design of the optimization procedure.

First, the three contributors in the training data are centered by the global median and scaled according to the global interquartile range, as these quantities are more robust than the mean and variance and scaling has been shown to stabilize training. Second, the model is trained with stochastic gradient descent. Although this is computationally less efficient than mini-batch gradient descent for such large training datasets, initial tests showed vanishing gradients when using batches leading to constant forecasts. Third, the Huber-loss is employed to leverage the benefits of the mean-squared-error and mean-absolute-error loss: improved stability near zero and decreased sensitivity to extreme values, respectively. The Huber-loss threshold is set to its typical value 1. Fourth, generalization is improved by using L2-regularization that has been shown to be particularly beneficial for prediction tasks on multivariate time series \citep{zhou_explore_2019}. It penalizes the magnitude of model weights to ensure that the resulting function is less sensitive to small perturbations, thus limiting over-fitting. Fifth, a dropout layer is introduced between the LSTM and fully connected layers to further promote model generalization. 

Then, given network output $\mathbf{\hat{y}} = (\mathbf{\hat{y}}_{\text{adv}}, \mathbf{\hat{y}}_{\text{adiab}}, \mathbf{\hat{y}}_{\text{diab}})$ and truth $\mathbf{y} = (\mathbf{y}_{\text{adv}}, \mathbf{y}_{\text{adiab}}, \mathbf{y}_{\text{diab}})$, model parameters $\mathbf{W}$, the L2-regularization hyper-parameter $r$ and Huber-loss threshold $\delta$, the objective function is given by:

\begin{equation}
  \begin{aligned}
   \text{obj}(\mathbf{\hat{y}},\mathbf{y}) & = \frac{1}{24}\sum_{i=1}^{24} l(\hat{y}_i,y_i) + r \cdot || \mathbf{W} ||_2 \\
   l(\hat{y},y) & = \begin{cases} 0.5(y - \hat{y})^2, & \text{ if } |y - \hat{y}| < \delta \\
   \delta \left( |y - \hat{y}| -0.5\delta \right), & \text{ otherwise } \end{cases}
   \end{aligned}
(\#eq:obj)
\end{equation}

where $||\cdot ||_2$ is the L2-norm and $l_i$ the Huber-loss for one prediction.

```{r traintest, fig.cap="Training and validation splitting scheme. Years between 1980 and 2000 are used for training (green shaded areas) and validation (red shaded areas). The remaining years 2001 to 2020 and the whole globe are used for inference.", echo=FALSE, out.width="80%", fig.pos = 'h', fig.align='center'}
knitr::include_graphics("images/test_train_fig.png")
```

Due to the high-resolution of ERA5 data, the dataset contains over 10 million timeseries. To select the model hyper-parameters (L2-regularization weight, LSTM hidden unit size), the globe is divided into 16 rectangular regions according to figure \@ref(fig:traintest): 8 for training, 8 for validation. In addition, the timeseries exhibit large spatial dependencies within a given year since they may share the same flow. Therefore, to have a more faithful estimation of generalization performance, the validation regions are reduced in latitude and longitude extent by a factor proportional to the maximum of the average trajectory distance in each region. Finally, to limit computational burden and perform inference on unseen data everywhere on the globe, only years from 1980 to 1990 are used for training.

The final model parameters are obtained by randomly sampling 50'000 timeseries ($\sim 4\%$ of all training data) and training a model instance for each combination of LSTM layer hidden unit size $[48,64,88]$ and L2-regularization weight $[0,10^{-6},10^{-4}]$. An estimate of generalization loss is computed on half of the validation data, randomly sampled. Each instance is trained with the Adam algorithm \citep{kingma_adam_2017}, a learning rate of $2\times 10^{-4}$ and for a maximum of 15 epochs, stopped early if validation loss does not improve within three epochs. LSTM hidden and cells states are re-initialized at every input with zero matrices. The parameters of the final model are chosen by considering low validation loss (see Appendix C.1 for results) relative to model complexity and expected behavior on a larger dataset. The final model is trained on the full training data and is used to forecast each test timeseries, consisting of all locations for years 1991 to 2020. To obtain a reference predictive performance, a simple constant model is created that outputs the latest input value for each contributor - i.e. the value at time 27h before the event. 
