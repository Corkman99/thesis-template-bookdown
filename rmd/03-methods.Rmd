# Methods

## Decomposition of TX1day anomalies and their yearly variance

To extend the study of mean-behaviour of TX1day events by M. Rothlisberger and L. Papritz (2023), we propose a variance decomposition of temperature anomaly that quantifies the contributions of physical processes on variability. For $n$ temperature anomaly samples $\{ T' \}_{1:n}$ assuming that ${T'}_i = \text{adv}_i +\text{adiab}_i +\text{diab}_i$, we have that:

\begin{equation}
\label{eq:var_decomp}
 \begin{split}
   V(\{ T' \}_{1:n})  & = V(\{ \text{adv} \}_{1:n} ) + V(\{ \text{adiab} \}_{1:n} ) + V(\{ \text{diab} \}_{1:n} ) + \\
   & \qquad Cov(\{ \text{adv} \}_{1:n} ,\{ \text{adiab} \}_{1:n}) + Cov(\{ \text{adv} \}_{1:n},\{ \text{diab} \}_{1:n}) + Cov(\{ \text{adiab} \}_{1:n},\{ \text{diab} \}_{1:n})
\end{split}
\end{equation}

Applying (@eq-var_decomp) to every location on Earth yields a direct and systematic approach to the analysis of global patterns of hot extreme variability in the presence of complex dependencies between physical processes. The linearity of the covariance operator and the fact that it is un-normalized, in comparison to correlation for instance, allows for a more intuitive comparison of second-moment contributions. 

## Principle Component Analysis

Principle Component Analysis (PCA), known as Empirical Orthogonal Functions in climate sciences, is a linear transformation introduced by K. Pearson (1901) and first applied in climate sciences for weather prediction by E. Lorenz (1956). Given multivariate samples of dimension $p$, it expresses data into $p$ orthogonal linear combinations of the original coordinates, determined iteratively to maximize explained variance. It is commonly used as a dimension reduction method by truncating to coordinates with high explained variance and in machine learning to yield data with uncorrelated features.

We employ PCA to investigate the dependence structure of the temperature anomaly decomposition. Similar to the variance decomposition, the transform is applied independently to series of yearly maxima TX1day events at every location on Earth with the aim of providing insights in spatial variability. 

## Baselines for trajectory data

As was earlier discussed, trajectory-based analyses are often carried out case-by-case and thus difficult to systematically apply for a global study. We propose leveraging the flexibility of machine learning models to investigate trajectories of yearly maxima TX1day decompositions. To compare the predictive performance of the state-of-the-art model introduced in the next section, X standard baseline models are employed: constant final-value, a Long Short-Term Memory (LSTM) network and a Gated Recurrent Unit (GRU) network. 

LSTMs and GRUs are types of Recurrent Neural Networks (RNN) using feedback loops to allow outputs of one cell to be the inputs of a subsequent cell. LSTMs were introduced by Hochreiter and Schmidhuber (1997) to solve vanishing gradients in applications with long-term dependencies. Each LSTM layer is composed of three gates - the input, forget and output gates - that are recurrently computed one timestep at a time to update temporal relationships, expressed in current and hidden states. This complex structure implies considerable model flexibility, but is prone to over-fitting. GRUs are introduced by ... to reduce the number of model parameters by utilizing only two gates per recurrence. Figure X illustrates the architecture and the relevant equations. 

These models are applied to timeseries obtained by averaging back-trajectories integrated from genesis $t_g$ to times $t_{g+1}$,  $t_{g+2}$, ...,  $t_{0}$, where the final time is the last timestep before the day of the hot extreme. RNN architectures learn temporal patterns by ... and we therefore augment the data by applying a sliding window to each timeseries. 

Talk about cross-validation technique or parameter selection. 

Talk about used loss function

Talk about validation: we want it to be good at predicting 4 steps forward, but especially the final timesteps. Maybe a weighted loss? Could use this: 
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7038523/

## Shapelet-based neural networks