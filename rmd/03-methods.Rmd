# Methods

## Decomposition of TX1day anomalies and their yearly variance

To this end, we focus on investigating yearly maxima of 1-day temperature anomaly averages, denoted TX1day. Using anomalies, rather than magnitudes, leads to a more natural interpretation of the components' contribution to extreme warming, since some processes of interest, such as warming from horizontal advection, arise due to climatological differences *** (explain better). The variation in TX1day and its relation to underlying physical processes is captured by using the decomposition of TX1day anomalies into contributions from horizontal advection, adiabatic warming and diabatic warming (1), according to M. Rothlisberger and L. Papritz (2023). These quantities are now referred to as the advective (adv), adiabatic (adiab) and diabatic (diab) temperature anomalies. The decomposition is derived with a Lagrangian perspective, meaning that it diagnostically evaluates the evolution of a parcel of air along its trajectory in 3D space and time.

At location $\mathbf{x}$ and some time $t_X$, the temperature anomaly $T'$ may be decomposed as follows:

\begin{equation}
\label{eq:t_decomp}
 \begin{split}
   T'(\mathbf{x},t_X)  & = - \int_{t_g}^{t_X} \frac{\partial \bar{T}}{\partial t} \, \text{d}\tau - \int_{t_g}^{t_X} \mathbf{\nu} \mathbf{\nabla}_h \bar{T} \, \text{d}\tau + \int_{t_g}^{t_X} \left[ \frac{\kappa T}{p} - \frac{\partial \bar{T}}{\partial p}\right] \omega \, \text{d}\tau + \int_{t_g}^{t_X} \left( \frac{p}{p_0} \right)^\kappa \frac{\text{D}\theta}{\text{D}t} \, \text{d}\tau   \\
   & = \qquad \text{seas} \qquad \, + \, \quad \quad \text{adv} \qquad \, \, + \qquad \qquad \text{adiab} \qquad \quad \,  + \qquad \quad \text{diab} 
\end{split}
\end{equation}

where $\bar{T}$ the temperature climatology; $t_g < t_X$ genesis time, the time at which $T'$ was last zero; $\mathbf{\nu}$ the horizontal wind velocity; $\mathbf{\nabla}_h$ the horizontal temperature gradient; $p$ pressure; $\omega$ vertical velocity; $\theta$ potential temperature. 

To apply this decomposition for the study of hot extremes, LAGRANTO is employed to compute back-trajectories of air parcels found near the surface of a location of a hot extreme event. Given data described section 3, for each year at each location, a total of 24 back-trajectories are obtained corresponding to the 3 lower model-levels for 8 timesteps with 3 hour intervals. Each trajectory is computed until genesis $t=t_g$ of the temperature anomaly of the parcel, up to a maximum of 120 time-steps. The final dataset is created by evaluating the integrals for each of the 24 trajectories and taking an average of the final decomposed temperature anomaly.

Evaluating the integrals involves two errors arising from discrete approximations of the continuous time domain: error 1 denotes the deviation from zero of $T'(\cdot ,t_g)$; error 2 denotes the remainder of the components, seasonality and error 1 subtracted from $T'$. These errors are usually small relative to hot extreme $T'$ magnitudes and are thus dropped from further analysis. In addition, since TX1day events evolve on sub-seasonal timescales, the seasonality term (seas) is small and is also dropped. These decisions are supported by Appendix A. 

To extend the study of mean-behaviour of TX1day events by M. Rothlisberger and L. Papritz (2023), we propose a variance decomposition of temperature anomaly that quantifies the contributions of physical processes on variability. For $n$ temperature anomaly samples $\{ T' \}_{1:n}$ assuming that ${T'}_i = \text{adv}_i +\text{adiab}_i +\text{diab}_i$, we have that:

\begin{equation}
\label{eq:var_decomp}
 \begin{split}
   V(\{ T' \}_{1:n})  & = V(\{ \text{adv} \}_{1:n} ) + V(\{ \text{adiab} \}_{1:n} ) + V(\{ \text{diab} \}_{1:n} ) + \\
   & \qquad Cov(\{ \text{adv} \}_{1:n} ,\{ \text{adiab} \}_{1:n}) + Cov(\{ \text{adv} \}_{1:n},\{ \text{diab} \}_{1:n}) + Cov(\{ \text{adiab} \}_{1:n},\{ \text{diab} \}_{1:n})
\end{split}
\end{equation}

Applying (@eq-var_decomp) to every location on Earth yields a direct and systematic approach to the analysis of global patterns of hot extreme variability in the presence of complex dependencies between physical processes. The linearity of the covariance operator and the fact that it is un-normalized, in comparison to correlation for instance, allows for a more intuitive comparison of second-moment contributions. 

## Principle Component Analysis

Principle Component Analysis (PCA), known as Empirical Orthogonal Functions in climate sciences, is a linear transformation introduced by K. Pearson (1901) and first applied in climate sciences for weather prediction by E. Lorenz (1956). Given multivariate samples of dimension $p$, it expresses data into $p$ orthogonal linear combinations of the original coordinates, determined iteratively to maximize explained variance. It is commonly used as a dimension reduction method by truncating to coordinates with high explained variance and in machine learning to yield data with uncorrelated features.

We employ PCA to investigate the dependence structure of the temperature anomaly decomposition. Similar to the variance decomposition, the transform is applied independently to series of yearly maxima TX1day events at every location on Earth with the aim of providing insights in spatial variability. 

## Baselines for trajectory data

As was earlier discussed, trajectory-based analyses are often carried out case-by-case and thus difficult to systematically apply for a global study. We propose leveraging the flexibility of machine learning models to investigate trajectories of yearly maxima TX1day decompositions. To compare the predictive performance of the state-of-the-art model introduced in the next section, X standard baseline models are employed: constant final-value, a Long Short-Term Memory (LSTM) network and a Gated Recurrent Unit (GRU) network. 

LSTMs and GRUs are types of Recurrent Neural Networks (RNN) using feedback loops to allow outputs of one cell to be the inputs of a subsequent cell. LSTMs were introduced by Hochreiter and Schmidhuber (1997) to solve vanishing gradients in applications with long-term dependencies. Each LSTM layer is composed of three gates - the input, forget and output gates - that are recurrently computed one timestep at a time to update temporal relationships, expressed in current and hidden states. This complex structure implies considerable model flexibility, but is prone to over-fitting. GRUs are introduced by ... to reduce the number of model parameters by utilizing only two gates per recurrence. Figure X illustrates the architecture and the relevant equations. 

These models are applied to timeseries obtained by averaging back-trajectories integrated from genesis $t_g$ to times $t_{g+1}$,  $t_{g+2}$, ...,  $t_{0}$, where the final time is the last timestep before the day of the hot extreme. RNN architectures learn temporal patterns by ... and we therefore augment the data by applying a sliding window to each timeseries. 

Talk about cross-validation technique or parameter selection. 

Talk about used loss function

Talk about validation: we want it to be good at predicting 4 steps forward, but especially the final timesteps. Maybe a weighted loss? Could use this: 
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7038523/

## Shapelet-based neural networks