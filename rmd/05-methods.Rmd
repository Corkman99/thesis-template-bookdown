# Methods

## Decomposition of TX1day anomalies and their yearly variance

The aim of this thesis is to gain further understanding in the influence of the three $T'$-generating mechanisms on TX1day events, particularly relating to the variability in yearly maxima observations. Then, to extend the first-moment characterization of TX1day events by M. Rothlisberger and L. Papritz (2023), we propose a variance decomposition of $T'$ that quantifies the contributions of physical processes on variability. 

Recalling \@ref(eq:tdecomp), the evaluation of the integrals involves two errors arising from discrete approximations of the continuous time domain: error 1 denotes the deviation from zero of $T'(\cdot ,t_g)$; error 2 denotes the remainder of the components, seasonality and error 1 subtracted from $T'$. These errors are usually small relative to hot extreme $T'$ magnitudes and are thus dropped in the proposed decomposition. In addition, since TX1day events evolve on sub-seasonal timescales, the seasonality term (seas) is small and is also dropped \citep[see Extended Data in ][]{rothlisberger_quantifying_2023}. 

Then, for $n$ temperature anomaly samples $\{ T' \}_{1:n}$ assuming that ${T'}_i = \text{adv}_i +\text{adiab}_i +\text{diab}_i$, we have that:

\begin{equation}
   V(\{ T' \}_{1:n}) = \sum_{c \in \{\text{adv},\text{adiab},\text{diab}\}} V(\{ c \}_{1:n} ) + \sum_{c, d \in \{\text{adv},\text{adiab},\text{diab}\} , c\neq d} \text{Cov}(\{ c \}_{1:n} ,\{ d \}_{1:n})
(\#eq:vardecomp)
\end{equation}

Applying \@ref(eq:vardecomp) to every location on Earth yields a direct and systematic approach to the analysis of global patterns of hot extreme variability in the presence of complex dependencies between physical processes. The linearity of the covariance operator and the fact that it is un-normalized, in comparison to correlation for instance, allows for an intuitive comparison of second-moment contributions. I argue that considering the variance of the contributors without adjusting for the influence of other contributors due to existing correlation is more informative. For example, a large variation in adiabatic $T'$ contribution is important to report, regardless of the additional influence of advective and diabatic processes; if the sum of adiabatic and advective $T'$ has small variance, it will be reflected in a large negative covariance between the contributors. The limitation of using an un-normalized quantity is the fact that a large mean magnitude will in most cases imply a larger variance. Therefore, meaningful interpretation of second-moment estimates will also require consideration of first-moment estimates.

To summarize the spatial structure of TX1day $T'$ variance and its decomposition, variance terms will be ranked with respect to their magnitude according to the following definition (applied to both mean and variance decomposition data): a contributor is dominant if it is at least twice as large as the second largest contributor; two contributors are dominant if they are both at least twice as large as the remaining contributor; else, no contributor is dominant. As both meand and variance decompositions are additive, this importance definition allows for a intuitive comparison. 

The decision to use \@ref(eq:vardecomp) instead of other importance measures outlined in [Predictor importance] is motivate by the non-trivial data structure (see [Data]). By assuming residuals to be zero, the response and predictors are linearly dependent - in addition to strong negative correlations between contributors due to inherent physical constraints. This then limits the applicability of standard regression methods and the associated measures of predictor importance that assume them uncorrelated \citep{hooker_unrestricted_2021}. Addressing the constraint that the response is the sum of the three contributors, for example by transforming the contributors to a simplex space \citep{greenacre_compositional_2021}, is also problematic as the data does not follow the positivity constraint: contributions to T' may be negative. Pre-processing the data to enforce positivity leads to poor representation of the domain and hinders interpretation. Furthermore, we do not propose more robust scale measures - such as truncated variance - in order to preserve information about tail events that are often associated with the largest human impact. 

## Principle Component Analysis

Strong inter-contributor correlations limit the interpretability of the above method, but are important in understanding the physical mechanisms associated with hot extreme development and their constraints. To characterize the dependence structure between the contributors, PCA is employed to first assess the degrees of freedom of the system by computing proportion of explained variance for each PC and second to relate the directions of the PCs to the contributors. It is applied to the 41 centered and normalized samples of advective, adiabatic and diabatic T' contributions at every location on Earth and is computed exactly using Singular Value Decomposition as proposed by \cite{halko_finding_2011}. PCA is preferred over machine-learning approaches because of the small sample sizes at each location, it is variance-based and therefore can be related to previous work, and has an intuitive interpretation.

PCA scores are categorized into the 7 categories, similarly to the dominance definition introduced in [6.1][Decomposition of TX1day anomalies and their yearly variance]. Since the scores are unit-vectors in 3D space, the vector created by applying element-wise absolute-value lies on the positive octant of the surface of a unit sphere. This surface is partitioned into three corner regions (each contributor), the center (combination of all contributors) and three remaining surface divided evenly (pair-combinations), according to figure \@ref(fig:pcascores).

```{r pcascores, fig.cap="Illustration of the partitioning of surface of PCA absolute-value scores into 7 categories: advective (A), adiabatic (B), diabatic (C), advective-adiabatic (D), adiabatic-diabatic (E), diabatic-advective (F) and all (D). The construction points all lie on planes parallel to the axes at values $\\sqrt{2}/2$, $0.5$ and $\\sqrt{3}/2$ that correspond to 30, 45 and 60$^\\circ$ angles between the axial planes.", echo=FALSE, out.width="80%", fig.pos = 'h', fig.align='center'}
knitr::include_graphics("images/pca_partition.png")
```

## Forecasting TX1day trajectories

Trajectory-based analyses for hot extreme research are often carried out case-by-case. I propose leveraging the flexibility of DL models to investigate the predictability of yearly maxima TX1day decomposition trajectories. A global LSTM-based DL model is trained to forecast the final 8 timesteps (1 day) of each component of the TX1day event decomposition trajectories given the first 112 timesteps. The model is composed of a single LSTM layer followed by a fully-connected linear layer yielding 24 outputs (3 variables x 8 timesteps) and trained via gradient descent with the mean-squared error (MSE) loss. Since LSTMs are known to be flexible models prone to over-fitting, L2-regularization - which penalizes the magnitude of model weights - is a common method to limit over-fitting and promote model generalization, particularly beneficial for prediction tasks on multivariate time series \citep{zhou_explore_2019}. Given network output $\mathbf{\hat{y}} = (\mathbf{\hat{y}}_{\text{adv}}, \mathbf{\hat{y}}_{\text{adiab}}, \mathbf{\hat{y}}_{\text{diab}})$ and truth $\mathbf{y} = (\mathbf{y}_{\text{adv}}, \mathbf{y}_{\text{adiab}}, \mathbf{y}_{\text{diab}})$, model parameters $\mathbf{W}$ and some weight hyperparameter $r$, the objective function is given by:

\begin{equation}
   \text{obj}(\mathbf{\hat{y}},\mathbf{y}) = \frac{1}{24}\sum_{i=1}^{24} (y_i - \hat{y}_i)^2 + r \cdot || \mathbf{W} ||_2 
(\#eq:obj)
\end{equation}

where $||\cdot ||_2$ is the L2-norm. 

```{r traintest, fig.cap="Training and validation splitting scheme. Years between 1980 and 2000 are used for training (green shaded areas) and validation (red shaded areas). The remaining years 2001 to 2020 and the whole globe are used for inference.", echo=FALSE, out.width="80%", fig.pos = 'h', fig.align='center'}
knitr::include_graphics("images/test_train_fig.png")
```

Due to the high-resolution of ERA5 data and considering 40 years of data, the dataset contains over 10 million timeseries. The flattened globe is divided into 16 rectangular regions according to Figure \@ref(fig:traintest): 8 will be used for training, and 8 for validation. The timeseries exhibit large spatial dependencies within a given year since they may share the same flow. Therefore, to further promote a more general model, the validation regions are reduced in latitude and longitude extent by a factor proportional to the maximum of the average trajectory distance in each region. Furthermore, to limit computational burden and allow inference on unseen data everywhere on the globe, only years from 1980 to 2000 are used for training. The data is neither scaled nor normalized. 

A single model instance is trained for each combination of LSTM layer hidden unit size [64,128,256] and L2-regularization weight [0.00001,0.0001,0.001] with mini-batch gradient descent using the Adam algorithm \citep{kingma_adam_2017}. Each instance is trained with a learning rate of X and for a maximum of 5 epochs, stopped early if validation loss increases. LSTM hidden and cells states are re-initialized at every batch with zeros. To encourage diverse prediction behaviours, a low batch-size of X is used on the shuffled training data. The instance achieving the lowest objective \@ref(eq:obj) is chosen for inference. 

