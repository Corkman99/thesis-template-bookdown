# Statistical methods

## Decomposition of TX1day anomalies and their yearly variance

The aim of this thesis is to gain further understanding in the influence of the three $T'$-generating mechanisms on TX1day events, particularly relating to the variability in yearly maxima observations. The ideal output of a statistical analysis would then be quantifying the influence of each contributor on a second-moment estimate of yearly maxima TX1day magnitude at each location. 

Ranking the influence a certain predictor has on a response compared to other predictors is a common problem in applied statistics. Although $R^2$ is often reported to provide comparisons of model fit and thus measure the value of certain predictors in explaining the variance of the response, it does not take into account interactions between predictors. Instead, in the context of linear regression models, \cite{lindeman_introduction_1980} - referred to as LMG - propose to quantify predictor importance by computing the average sequential sums-of-squares over all predictor orderings. Averaging over all orderings allows this measure to capture both direct effects and secondary effects of a predictor. \cite{feldman_relative_2005} extended this approach with Proportional Marginal Variance Decomposition (PMVD) that instead considers a weighted average with weights chosen to enforce the exclusion criteria - a predictor with zero coefficient is allocated zero importance \citep{gromping_estimators_2007}. \cite{breiman_random_2001}, and extensions thereof, instead propose to measure importance of a predictor by assessing the model's loss of accuracy due to random permutations of the predictor values. 

This methods have however been shown to be susceptible to data with highly correlated predictors, as is often the case in observational settings lacking causal assumptions. Furthermore, with many predictors, LMG and PMVD become computationally intensive, as they require computation of $R^2$ values over all possible orderings. Permutation-based methods scale linearly with the number of predictors but may require running many permutations of each column to output estimates with low variance.

[Chapter 3][Data] has indeed demonstrated strong negative correlations between contributors that are due to inherent physical constraints. In addition, by assuming residuals to be approximately zero, the response and predictors are linearly dependent. This then limits the applicability of standard regression methods and the associated measures of predictor importance that assume them uncorrelated \citep{hooker_unrestricted_2021}. Addressing the constraint that the response is the sum of the three contributors, for example by transforming the contributors to a simplex space \citep{greenacre_compositional_2021}, is also problematic as the data does not follow the positivity constraint: contributions to $T'$ may be negative. Pre-processing the data to enforce positivity leads to poor representation of the domain and hinders interpretation. Furthermore, more robust scale measures - such as truncated variance - would not appropriate as information of tail events that are often associated with the largest human impact needs to be preserved.

Then, to extend the first-moment characterization of TX1day events by \cite{rothlisberger_quantifying_2023}, I propose a variance decomposition of $T'$ that quantifies the contributions of physical processes to variability. Recalling \@ref(eq:tdecomp) and assuming residuals and the seasonality term to be zero, the final $T'$ of the event is related linearly with no bias to the $T'$ generated by the three processes. For $n$ independent events achieving temperature anomalies $\{ T' \}_{1:n}$, data is then described by ${T'}_i = \text{adv}_i +\text{adiab}_i +\text{diab}_i$, and thus:

\begin{equation}
   V(\{ T' \}_{1:n}) = \sum_{c \in \{\text{adv},\text{adiab},\text{diab}\}} V(\{ c \}_{1:n} ) + \sum_{c, d \in \{\text{adv},\text{adiab},\text{diab}\} , c\neq d} \text{Cov}(\{ c \}_{1:n} ,\{ d \}_{1:n})
(\#eq:vardecomp)
\end{equation}

Applying \@ref(eq:vardecomp) to every location on Earth yields a direct and systematic approach to the analysis of global patterns of hot extreme variability in the presence of complex dependencies between physical processes. The linearity of the covariance operator and the fact that it is un-normalized, in comparison to correlation for instance, allows for an intuitive comparison of second-moment contributions. I argue that considering the variance of the contributors without adjusting for the influence of other contributors due to existing correlation is more informative. For example, a large variation in adiabatic $T'$ contribution is important to report, regardless of the additional influence of advective and diabatic processes; if the sum of adiabatic and advective $T'$ has small variance, it will be reflected in a large negative covariance between the contributors. The limitation of using an un-normalized quantity is the fact that a large mean magnitude will in most cases imply a larger variance. Therefore, meaningful interpretation of second-moment estimates will also require consideration of first-moment estimates.

To summarize the spatial structure of TX1day $T'$ variance and its decomposition, variance terms will be ranked with respect to their magnitude according to the following definition (applied to both mean and variance decomposition data): a contributor is dominant if it is at least twice as large as the second largest contributor; two contributors are dominant if they are both at least twice as large as the remaining contributor; else, no contributor is dominant. As both mean and variance decompositions are additive, this importance definition allows for a intuitive comparison. 

## Principle Component Analysis

As discussed, strong inter-contributor correlations limit the interpretability of the variance decomposition, but are important in understanding the physical mechanisms associated with hot extreme development and their constraints. To characterize the dependence structure between the contributors, Principal Component Analysis (PCA) is employed to assess the degrees of freedom of the heat-generating systems and provide an alternative importance definition to rank the contributors at each location. 

PCA - known as Empirical Orthogonal Functions in climate sciences - is a linear transformation introduced by \cite{pearson_liii_1901} and first applied in climate sciences for weather prediction by \cite{lorenz_empirical_1956}. It is commonly used as a dimension reduction method - to obtain a low-dimensional representation of high-dimensional inputs - and in identifying and quantifying modes of variability such as the North-Atlantic Oscillation and the El Nino-Southern Oscillation. The PCA problem consists in finding a projection $\mathbf{f}(\mathbf{x})$ that yields lower dimensional and orthogonal representations of the data that retain the most information.

Given $n$ mean-centered multivariate samples $\mathbf{X} = (\mathbf{x}_1, ..., \mathbf{x}_n) \in \mathbb{R}^{n \times p}$ of dimension $p$, the solution to obtain PCA transformation $\mathbf{W} = (\mathbf{w}_{(1)} | ... | \mathbf{w}_{(l)} ) \in \mathbb{R}^{l \times n}$, $1 \leq l \leq p$ and $l-$dimensional representations - or Principle Components (PC) - $\mathbf{z}_1, ..., \mathbf{z}_{n} \in \mathbb{R}^l$ is:

\begin{equation}
(\mathbf{W}, \mathbf{z}_1, ..., \mathbf{z}_{n}) = \argmin_{\mathbf{W}^{\intercal}\mathbf{W}=\mathbf{I}_l,\mathbf{z}} \, \sum_{i=1}^n \left|\left| \mathbf{W}\mathbf{z}_i - \mathbf{x}_i \right|\right|^2_2
(\#eq:pcatrans)
\end{equation}

with $f(\mathbf{x}_i) = \mathbf{z}_i = \mathbf{W}^{\intercal} \mathbf{x}_i$. The weights $(\mathbf{w}_{(1)} | ... | \mathbf{w}_{(l)} )$ are related to the empirical covariance matrix $\Sigma$ of $\mathbf{X}$ as they are its eigenvectors corresponding to its eigenvalues $(\lambda_{(1)}, ..., \lambda_{(p)})$ ranked by decreasing magnitude: 

\begin{equation}
\Sigma = \sum_{i=1}^p \lambda_{(i)} \mathbf{w}_i \mathbf{w}_i^{\intercal} 
(\#eq:pcacov)
\end{equation}

The first PC (PC1) then points in the direction with the largest variance, PC2 points in the direction orthogonal to PC1 with the largest variance, etc. The proportion of variance explained by a single PC can be computed using the eigenvalues: 

\begin{equation}
\text{\% explained variance }(\text{PC}i) = \frac{\lambda_{(i)}}{\lambda_{(1)}+...+\lambda_{(p)}}
(\#eq:pcaexplvar)
\end{equation}

PCA is then applied to the advective, adiabatic and diabatic $T'$ contributions of the 41 events at each location. To present an alternative measure of importance than the variance decomposition, each contributor is centered and scaled independently. This focuses the analysis on the contributor dependencies since it compares the variability structure without the influence of absolute magnitudes. The components are computed exactly using Singular Value Decomposition as proposed by \cite{halko_finding_2011} and the analysis will consider the proportion of explained variance for each PC and relate the resulting directions of the PCs to the contributors. 

PCA scores are categorized into the 7 categories, similarly to the dominance definition introduced in [6.1][Decomposition of TX1day anomalies and their yearly variance]. Since the scores are unit-vectors in 3D space, the vector created by applying element-wise absolute-value lies on the positive octant of the surface of a unit sphere. This surface is partitioned into three corner regions (each contributor), the center (combination of all contributors) and three remaining surface divided evenly (pair-combinations), according to figure \@ref(fig:pcascores).

```{r pcascores, fig.cap="Illustration of the partitioning of surface of PCA absolute-value scores into 7 categories: advective (A), adiabatic (B), diabatic (C), advective-adiabatic (D), adiabatic-diabatic (E), diabatic-advective (F) and all (D). The construction points all lie on planes parallel to the axes at values $\\sqrt{2}/2$, $0.5$ and $\\sqrt{3}/2$ that correspond to 30, 45 and 60$^\\circ$ angles between the axial planes.", echo=FALSE, out.width="80%", fig.pos = 'h', fig.align='center'}
knitr::include_graphics("images/pca_partition.png")
```

Finally, PCA is preferred over machine-learning approaches because of the small sample sizes at each location, it is variance-based and therefore can be related to previous work, and has an intuitive interpretation.

## Forecasting TX1day trajectories

Trajectory-based analyses for hot extreme research are often carried out case-by-case. I propose leveraging the flexibility of Deep-Learning (DL) models to investigate the predictability of yearly maxima TX1day decomposition trajectories. **TALK ABOUT WHY WE DO THIS**

Machine learning and in particular DL architectures have recently received a lot of attention in atmospheric and climate sciences due to their performance in modelling complex high-dimensional systems and their computational efficiency in comparison to numerical weather prediction. DL models have largely been used to develop weather forecasting models \citep[see for example][]{srivastava_weather_2022} and to learn parameterizations from simulated and observational datasets \citep{barahona_deep_2023}. I first provide a brief introduction to a DL architecture that has been used extensively in timeseries applications. 

Supervised DL involves the parameterization of arbitrarily complex functions - called networks - and estimation of these parameters by propagating input data through the network and evaluating the outputs' accuracy against the given truth - called the loss. By using a differentiable loss and network, the parameters can be updated. With sufficient data and learning iterations, the model parameters will be progressively updated to improve accuracy. 

The modelling and processing of sequential data requires specific architectures such as Recurrent Neural Networks (RNNs) that can leverage dependencies such as auto-correlations. Long Short-Term Memory (LSTM) cells are a type of RNN that were introduced by \cite{hochreiter_long_1997} to solve the problem of vanishing gradients - when parameter updates become zero due to decreasing gradient magnitudes - in applications with long-term temporal dependencies. An LSTM layer is composed of a number of LSTM cells, each composed of three **nodes (i dont like this, rephrase to be more intuitive)**  called gates: the input, forget and output gates. Given a sequence of data, each unit recurrently propagates a data sequence one timestep at a time to update the gate parameters and layer parameters that store temporal information: the hidden states that are used to communicate information from one timestep to the next, and cell states that retain a memory of the sequence characteristics. More explicitly, given initial hidden $H_{t-1}$ and cell states $C_{t-1}$, an input $x_t$ at time-step $t$ is processed in the following way:

\begin{equation}
  \begin{aligned}
    k_t &= \sigma \left( W_k \cdot [H_{t-1}, x_t] + b_k \right) \\
    l_t &= \sigma \left( W_l \cdot [H_{t-1}, x_t] + b_l\right) \\
    \tilde{C}_t &= \tanh \left( W_{\tilde{C}} \cdot [ H_{t-1}, x_t ] + b_{\tilde{C}}\right) \\
    C_t &= k_t \ast C_{t-1} + l_{t} \ast \tilde{C}_t \\
    m_t &= \sigma \left( W_m \cdot [H_{t-1}, x_t] + b_m\right) \\
    H_t &= m_t \ast \tanh (C_t)
  \end{aligned}
(\#eq:lstm)
\end{equation}

where $\sigma$ and $\tanh$ are the sigma and tanh activation functions; $W_i$ and $b_i$ are matrices of gate parameters; $k_t,l_t$ and $m_t$ are the outputs of the forget, input and output gates respectively. A schematic representation of this process can be found in Figure \@ref(fig:lstm). 

```{r lstm, fig.cap="(a) Representation of an LSTM cell (in orange) that, given intitial hidden states $H_{t-1}$, cell states $C_{t-1}$ and an input $x_t$ outputs the updated hidden $H_{t}$ and cell $C_{t}$ states. Purple shapes represents operations: $\\ast$ matrix multiplication, $+$ element-wise addition, tanh the activation function, and the Forget, Input and Output gates are as define in equations 3.3. (b) The same LSTM cell but unfolded. For an input of initial hidden states $H_{0}$, cell states $C_{0}$ and data sequence $x_{1:n}$, it outputs hidden states $H_t$ that can then be further processed (e.g. by a fully connected layer).", echo=FALSE, out.width="80%", fig.pos = 'h', fig.align='center'}
knitr::include_graphics("images/lstm_fig.png")
```

LSTM layers may used in different ways depending on the data structure and task at hand. In timeseries forecasting applications, most models use the final hidden states $H_t$ of the last LSTM layers as lower-dimensional feature representations of every input that are then further processed by one or more fully-connected NN layers. There is a rich and evolving literature on LSTM-based architectures and LSTM variants and I refer the reader to comprehensive review papers such as \cite{hu_time_2020} and \cite{han_review_2021}.

Here, a global LSTM-based DL model is trained to forecast the final 8 timesteps (1 day) of each component of the TX1day event decomposition trajectories given the first 113 (14 days) timesteps. The model is composed of a single LSTM layer followed by a fully-connected linear layer yielding 24 outputs (3 variables x 8 timesteps) and trained via gradient descent with the mean-squared error (MSE) loss. LSTMs are known to be flexible models prone to over-fitting and the considered dataset is large and exhibits diverse behaviours. To allow the model to represent this diversity, two common methods are employed: a dropout layer and L2-regularization. Dropout promotes model generalisation by randomly masking the forward transfer of information on certain nodes at each training iteration. At inference, the full information is passed but is weighed inverse-proportionally the number of masked nodes. L2-regularisation penalizes the magnitude of model weights to ensure that the resulting function is less sensitive to small perturbations, thus limiting over-fitting. It has been found to be particularly beneficial for prediction tasks on multivariate time series \citep{zhou_explore_2019}. 

Then, given network output $\mathbf{\hat{y}} = (\mathbf{\hat{y}}_{\text{adv}}, \mathbf{\hat{y}}_{\text{adiab}}, \mathbf{\hat{y}}_{\text{diab}})$ and truth $\mathbf{y} = (\mathbf{y}_{\text{adv}}, \mathbf{y}_{\text{adiab}}, \mathbf{y}_{\text{diab}})$, model parameters $\mathbf{W}$ and some weight hyperparameter $r$, the objective function is given by:

\begin{equation}
   \text{obj}(\mathbf{\hat{y}},\mathbf{y}) = \frac{1}{24}\sum_{i=1}^{24} (y_i - \hat{y}_i)^2 + r \cdot || \mathbf{W} ||_2 
(\#eq:obj)
\end{equation}

where $||\cdot ||_2$ is the L2-norm. 

```{r traintest, fig.cap="Training and validation splitting scheme. Years between 1980 and 2000 are used for training (green shaded areas) and validation (red shaded areas). The remaining years 2001 to 2020 and the whole globe are used for inference.", echo=FALSE, out.width="80%", fig.pos = 'h', fig.align='center'}
knitr::include_graphics("images/test_train_fig.png")
```

Due to the high-resolution of ERA5 data and considering 40 years of data, the dataset contains over 10 million timeseries. The flattened globe is divided into 16 rectangular regions according to Figure \@ref(fig:traintest): 8 will be used for training, and 8 for validation. The timeseries exhibit large spatial dependencies within a given year since they may share the same flow. Therefore, to further promote a more general model, the validation regions are reduced in latitude and longitude extent by a factor proportional to the maximum of the average trajectory distance in each region. Furthermore, to limit computational burden and allow inference on unseen data everywhere on the globe, only years from 1980 to 2000 are used for training. The data is neither scaled nor normalized. 

A single model instance is trained for each combination of LSTM layer hidden unit size [64,128,256] and L2-regularization weight [0.00001,0.0001,0.001] with mini-batch gradient descent using the Adam algorithm \citep{kingma_adam_2017}. Each instance is trained with a learning rate of X and for a maximum of 5 epochs, stopped early if validation loss increases. LSTM hidden and cells states are re-initialized at every batch with zeros. To encourage diverse prediction behaviours, a low batch-size of X is used on the shuffled training data. The instance achieving the lowest objective \@ref(eq:obj) is chosen for inference. 

