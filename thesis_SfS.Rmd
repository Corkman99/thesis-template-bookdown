--- 
title: "A bookdown template for sfs"
author: "Nicola Gnecco and Lorenz Walthert"
date: "April 2018"
submission_date: "August 1th 2018"
adviser: "Prof. Dr. Nicolai Meinshausen"
coadviser:
output:
  bookdown::epub_book:
    default
  bookdown::pdf_book:
    template: tex/MasterThesisSfS.tex
    keep_tex: yes
    pandoc_args: --top-level-division=chapter
    citation_package: natbib
    latex_engine: pdflatex
    toc_depth: 3
    toc_unnumbered: no
    toc_appendix: yes
    toc_bib: yes
    quote_footer: ["\\begin{flushright}", "\\end{flushright}"]
    highlight: 
      tango
    includes:
      in_header: tex/preamble.tex
  bookdown::gitbook: default
bibliography: [bib/bib.bib]
biblio-style: apalike
link-citations: yes
colorlinks: no  
lot: no
lof: no
site: bookdown::bookdown_site
---

---
#[comment]: <> (You'd usually put a preface here and get rid of the warning:
#Warning message:
#In split_chapters(output, gitbook_page, number_sections, split_by,  :
#  You have 8 Rmd input file(s) but only 7 first-level heading(s). Did you 
#  forget first-level headings in certain Rmd files?
#)
---
 

<!--chapter:end:index.Rmd-->

---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "figures/README-",
  out.width = "100%"
)
```

# A few words from the authors {-}

This repository is a [bookdown](https://github.com/rstudio/bookdown) template, 
derived from the LaTeX template from the seminar for statistics, ETH Zurich.

**How bookdown works**

In a nutshell, bookdown works as follows:

* use knitr to convert Rmd to markdown.
* use pandoc to convert markdown to latex, pdf, word, html books (with features
  like font size, background selection, full text search etc), epub.

The basic workflow in RStudio is as follows:

* Open the RStudio project.
* Change a source file: In our template, the source of the body of the thesis
  is under `./rmd/`. For example, change `02-features.Rmd`. To make sure only
  files from this directory are used, we set the option `rmd_subdir: ["rmd/"]`
  in `bookdown.yml`. For this to work you need bookdown version 0.9 or greater.
* Re-compile the book using Cmd + Shift + B (for build) on a Mac and Ctrl + 
  Shift + B on Windows / Linux.
* You can customize the build in the RStudio Build Tab, where you can specify
  which output (html, pdf, etc.) you want to generate. This is remembered for the
  next build you are doing.

**Getting started**

For the git cracks: Fork the upstream repository and clone the fork.Then, you will have one remote
repository: origin, which refers to the fork. Add the upstream repo as an
upstream remote so you can later rebase on it in case you need. We recommend
using ssh over https, but if that's too complicated with the RSA key, just 
use https, it will do the job.

For everyone else: You can also simply download the repo.
However, we strongly recommend using version control for your thesis.

Make sure to [watch](https://help.github.com/en/articles/watching-and-unwatching-repositories)
this GitHub repo for issues that might affect you.

**Why using bookdown**

The advantages of using bookdown instead of plain LaTeX are, in the eyes of the
creator of this template:

* Generalization. Not just latex or PDF output, but any output supported by
  Pandoc, e.g. Word and html, epub. If you at some point decide to work with 
  LaTeX only, just render to LaTeX and continue. You don't loose anything 
  compared to status quo. When you host your source code on GitHub, you can 
  easily publish a html book that is updated with every push. How? Checkout 
  https://github.com/ropenscilabs/tic.bookdown.
* The best of two worlds. Use intuitive markdown syntax where possible, use the
  full power of LaTeX syntax where needed. This includes bibtex reference, LaTeX 
  cross-, text- and figure reference.
* It's native R. Integrate R code and R output such as plots, tables, figures
  directly in your writing has never been easier than with bookdown. You can turn
  a data frame into a latex table using the power of R packages such as
  `kableExtra`. You can change the data subset, re-compile the whole book and 
  all figures, tables and other data dependent elements will be updated.
* The power of RStudio. Use the same IDE for programming and writing. Leverage
  the advantages of a real-time latex  equation previewer right in your R
  Markdown, a git GUI, spell checking, file browser and more.
* You can also use other languages supported by knitr such as
  python, stan etc. You can even use R code to control the behavior of chunk 
  evaluation. Below, we even used the bash command `tree` to show the
  directory structure of this repo. Since unicode is not supported with pdflatex,
  we used R code to tell knitr to use the tree command with the option `charset
  unicode` for pdf output and without it for html output. Check the Rmd source 
  of the README to learn more.
* Because it's native R, you can place R variables in the floating text. Check
  out the Rmd source of this document to see that we used R to compute the square
  root of three (`r sqrt(3)`), print today's date with `Sys.Date()` right into the
  text: `r Sys.Date()`

**How this template works**

There are different directories in this template:

```{bash, include = knitr::is_html_output(), eval = FALSE}
tree -d -L 2
```

```{bash, include = !knitr::is_html_output(), eval = FALSE}
tree -d -L 2 --charset unicode
```

* _book: Contains the compiled book, e.g. a PDF or html version.
* _bib: Contains BibTeX reference data bases.
* figure: Contains figures you created from your (R) code in the rmd source.
* images: Put images you want to include in your thesis in this folder.
* pdf: Put PDFs you want to include in your thesis in this folder.
* rmd: The source folder of thesis. When you build a book, the following
  happens: By default, all rmd files in this folder get merged into one big rmd
  file, according to their name. We suggest to use one file per chapter. Then, the
  file containing all the rmd sources will get processed by knitr and later by
  Pandoc.
* scratch: A random directory which is not tracked by git by default. The idea
  is that you can put things that are not ready to commit or that have a temporary
  character here.
* style: Latex .sty files. Taken from the sfs LaTeX template.
* tex: All tex data, most importantly, the sfs LaTeX template itself. Note that
  the abstract, preface, epilogue, summary, notation are still in tex, i.e. you
  need to change these files if you want the PDF output to change. This will
  **not** affect the html output. You can create chapters that show up in a
  particular output and not in the other forms as shown in
  `rmd/99-references.Rmd`.

Furthermore, we want to highlight a few files in the root directory of the
project:

* `DESCRIPTION`: You can use it to declare dependent packages of your thesis in
  the `Imports:` field. If someone want's to rebuild you thesis from scratch, the
  can use `remotes::install_deps()` to satisfy all R package dependencies.
* `index.Rmd`: Contains a YAML header where a few important variables are
  defined. You can also put markdown below the header.
* `README.(R)md`: This document.
* `thesis-template-bookdown.Rproj`: An RStudio project. We recommend using
  RStudio for authoring your master thesis with bookdown.

**A few recommendations**

* We strongly suggest to use a different folder for the data / code of your
  thesis and reserve this directory for the thesis document only. You can rename
  this directory `analysis-communication` (since this will be the means used for
  communicating the results and process of your thesis) and use `analysis-raw` for
  the code and `data` for all data. Ideally, you place them in the same directory
  so you can still work with relative paths, e.g. `../data`.
* We suggest to use the package manager [{renv}](https://rstudio.github.io/renv/) 
  for better reproducibility and isolation for the packages in this project from 
  the global library. It's already initialised with this template. All packages
  you install and update are contained in a separate library. 
* We suggest to use git version control for the thesis and the raw analysis.
* If your thesis is open source, you can use netlify.com to deploy it, i.e. for
  every commit you push to a remote repo like GitHub, you can use netlify's CLI to
  build your book on a CI machine like travis. See file `travis.yml` for the
  bookdown book [Advanced
  R](https://github.com/hadley/adv-r/tree/88dcb07e2b2ae634af6cdeafff2f3ea976077064)
  for an example. That makes tracking the `_book` folder in git redundant.
* To keep lines 80 characters long, you can use the stylermd Rstudio Addin. 
  Assign a keyboard shortcut to it and it will format your text nicely.
  See https://github.com/lorenzwalthert/stylermd for details. 
* You might want to use [{targets}](https://docs.ropensci.org/targets/), a MAKE 
  like pipelining tool to keep your analysis reproducible and only re-execute 
  the outdated parts.


**Further material**

This is obviously a very short introduction to the template and it is in no way
comprehensive. To learn more about bookdown, we encourage the reader to have a
look at the [bookdown guide](https://bookdown.org/yihui/bookdown/) as well as
searching through stack overflow / Google for particular questions. In addition,
inspect the different files in this repo, in particular the ones in the rmd
directory to develop a deeper understanding of the template.

As of early 2018, this template is still in alpha testing phase, so you may
experience unexpected behavior. If you experience problems 
*after* you searched on Google and other platforms for a solution, feel free to 
open an issue in this repo.


Best,

Nicola Gnecco and Lorenz Walthert


<!--chapter:end:README.Rmd-->

# Introduction

## Motivation

In recent years, near-surface hot extremes have globally become increasingly more important. They impact human health with often high mortality rates in certain demographics (REF, REF, REF); can be damaging to agricultural and ecological systems through productivity decreases and heat stress (Teixeira, REF, REF); and may pressure national economies by increasing energy demands (Perera) compounded with power infrastructure damage (Entricken), as well as lowering labour productivity (Garcia-Leon). Furthermore, climate change is expected to exacerbate hot extreme impacts through an increase in the frequency of hot extremes as well as their intensity (R. Slater et al., 2021; E. Fischer et al. 2021; V. Thompson et al., 2022). There is thus a need for robust process understanding of the drivers of hot extremes to guide emergency planning and mitigation strategies. 

Globally, heat-waves and hot extreme events contribute the largest risks to human societies (maybe too strong, but can find a good source). 

Coupled with increasing mean global temperatures, recent record-shattering events highlight the importance in understanding not only the evolution and drivers of typical hot extremes, but also their variability. The Pacific North-West 2021 event is particularly illustrative: it was hard to predict with no ensemble member resolving such intense temperature 10 days before (Hai Lin et al., 2022); 

with models / forecasters not expecting temperatures that went above return period estimations (REF). Uncovering the differences that separate such events from lower-intensity hot extremes is helpful to understand how they may change under climate change. 

## Related work 

In order to present a holistic review of existing literature on hot extremes, their development is examined with three heat-generating mechanisms: advection involves the horizontal transport of warmer air into a cooler air mass; adiabatic heating arises from descending air that compresses and thus heats; diabatic heating involves radiative effects and sensible heat fluxes from the Earth's surface. The absence of the reverse mechanisms (cold air advection, ascending air and diabatic cooling) may also contribute to the formation of hot extremes. This characterisation providing a uniform basis to discuss atmospheric phenomena linked to hot extreme evolution and suggests a methodology, discussed in Methods, to quantify the contribution of these physical processes to specific events. 

In the mid-latitudes, the major weather regime influencing hot extremes is atmospheric blocking, a pattern characterized by persistent anticyclonic activity that prevents the influence of the prevailing flows (A. Lupo, 2020). Over Central and Western Europe, atmospheric blocking leads to regions of large subsidence and fair weather, further promoting strong radiative heating (L. Kautz, et al, 2022). Quantifying the co-location of atmospheric blocking and hot extremes has however shown large spatial variability both in the Northern (S. Pfahl and H. Wernli, 2012; L. Brunner et al., 2018) and Southern Hemisphere mid-latitudes (A. Pezza et al., 2011). WHAT ABOUT THE TROPICS. For polar latitudes, a case-study on the March 2022 East Antarctica heat extreme shows that intense atmospheric blocking led the weakening of the ice-sheet temperature inversion, intensifying adiabatic heating from subsiding air. 

Soil-dryness has been shown to be both a major driver of hot extreme events as well as exacerbate atmospheric blocking condition. Soil moisture deficit is understood to increase surface sensible heat fluxes, in turn "leading to diurnal convection favouring the entrainment of warm air, and the formation of deep and warm nocturnal residual layers allowed the heat to re-enter the mixed layer in the following days" (D. Miralles et al., 2014). M. Vogel et al. (2017) quantified these effects, finding that more than 70\% of increase in TX1day magnitude in Central Europe and Central North America, and around 50\% in Amazonia, Northern Australia and Southern Africa.

Marine heatwaves and TX1day events are understood to evolve ... E. Oliver et al., 2020

In addition to understanding the development of hot extremes, many studies have focused on the influence of multi- and inter-decadal variability. TO DO

Beyond understanding the mean behaviour of hot extremes, a limited number of studies have investigated the variability of hot extremes at local to regional spatial scales. Over Europe, E. Fischer and C. Schar (2008) quantified the changes in hot extreme variability with respect the processes associated with inter-annual variability, intra-seasonal variability and the seasonal variability of summer months. L. Suarez-Gutierrez et al. (2020) instead suggests attributing variability changes to dynamical and thermodynamical sources, concluding that dynamical mechanisms, encompassing effects of atmospheric blocking, are the main drivers of hot extreme variability. 

TALK ABOUT THE METHODOLOGIES USED IN OTHER EXTREME RESEARCH: 
A. Casanueva et al. (2014) propose investigating the variability of extreme precipitation events through large-scale teleconnection patterns, such as the Southern Oscillation and the North Atlantic Oscillation, by trend analysis and correlation analysis. 

The weather extremes literature has used two main methodologies: trajectory analysis such as Lagrangian tools (..., ..., ...) and sensitivity analysis. Trajectory analysis have been limited to 'manual' stuff (no ML) 

Although a good general understanding of hot extreme evolution exists and more detailed assessments have been made regionally and for specific high-impact events, there then exists a knowledge-gap in quantifying the contribution of known physical processes to hot extreme magnitude variability globally. This manuscripts attempts to close this gap by first proposing a systematic analysis of second-moment characteristics of hot extreme events and second applying state-of-the-art deep-learning methods to uncover evolutionary features influencing their magnitudes. 

<!--chapter:end:rmd/01-intro.Rmd-->

# Features

`bookdownplus` extends the features of `bookdown`, and simplifies the
procedure. Users only have to choose a template, clarify the book title and
author name, and then focus on writing the text. No need to struggle in YAML
and LaTeX.
With `bookdownplus` users can

-   record guitar chords,

-   write a mail in an elegant layout,

-   write a laboratory journal, or a personal diary,

-   draw a monthly or weekly or conference calendar,

-   and, of course, write academic articles in your favourite way,

-   with chemical molecular formulae and equations,

-   even in Chinese,

-   and more wonders will come soon.

Full documentation can be found in the book 
[R bookdownplus Textbook](https://bookdown.org/baydap/bookdownplus). The webpage 
looks so-so, while the 
[pdf file](https://bookdown.org/baydap/bookdownplus/bookdownplus.pdf) might give
you a little surprise.

<!--chapter:end:rmd/02-features.Rmd-->

# Methods


## Decomposition of TX1day anomalies and their yearly variance

To this end, we focus on investigating yearly maxima of 1-day temperature anomaly averages, denoted TX1day. Using anomalies, rather than magnitudes, leads to a more natural interpretation of the components' contribution to extreme warming, since some processes of interest, such as warming from horizontal advection, arise due to climatological differences *** (explain better). The variation in TX1day and its relation to underlying physical processes is captured by using the decomposition of TX1day anomalies into contributions from horizontal advection, adiabatic warming and diabatic warming (1), according to M. Rothlisberger and L. Papritz (2023). These quantities are now referred to as the advective (adv), adiabatic (adiab) and diabatic (diab) temperature anomalies. The decomposition is derived with a Lagrangian perspective, meaning that it diagnostically evaluates the evolution of a parcel of air along its trajectory in 3D space and time.

At location $\mathbf{x}$ and some time $t_X$, the temperature anomaly $T'$ may be decomposed as follows:

\begin{equation}
\label{eq:1}
 \begin{split}
   T'(\mathbf{x},t_X)  & = - \int_{t_g}^{t_X} \frac{\partial \bar{T}}{\partial t} \, \text{d}\tau - \int_{t_g}^{t_X} \mathbf{\nu} \mathbf{\nabla}_h \bar{T} \, \text{d}\tau + \int_{t_g}^{t_X} \left[ \frac{\kappa T}{p} - \frac{\partial \bar{T}}{\partial p}\right] \omega \, \text{d}\tau + \int_{t_g}^{t_X} \left( \frac{p}{p_0} \right)^\kappa \frac{\text{D}\theta}{\text{D}t} \, \text{d}\tau   \\
   & = \qquad \text{seas} \qquad \, + \, \quad \quad \text{adv} \qquad \, \, + \qquad \qquad \text{adiab} \qquad \quad \,  + \qquad \quad \text{diab} 
\end{split}
\end{equation}

where $\bar{T}$ the temperature climatology; $t_g < t_X$ genesis time, the time at which $T'$ was last zero; $\mathbf{\nu}$ the horizontal wind velocity; $\mathbf{\nabla}_h$ the horizontal temperature gradient; $p$ pressure; $\omega$ vertical velocity; $\theta$ potential temperature. 

To apply this decomposition for the study of hot extremes, LAGRANTO is employed to compute back-trajectories of air parcels found near the surface of a location of a hot extreme event. Given data described section 3, for each year at each location, a total of 24 back-trajectories are obtained corresponding to the 3 lower model-levels for 8 timesteps with 3 hour intervals. Each trajectory is computed until genesis $t=t_g$ of the temperature anomaly of the parcel, up to a maximum of 120 time-steps. The final dataset is created by evaluating the integrals for each of the 24 trajectories and taking an average of the final decomposed temperature anomaly.

Evaluating the integrals involves two errors arising from discrete approximations of the continuous time domain: error 1 denotes the deviation from zero of $T'(\cdot ,t_g)$; error 2 denotes the remainder of the components, seasonality and error 1 subtracted from $T'$. These errors are usually small relative to hot extreme $T'$ magnitudes and are thus dropped from further analysis. In addition, since TX1day events evolve on sub-seasonal timescales, the seasonality term (seas) is small and is also dropped. These decisions are supported by Appendix A. 

To extend the study of mean-behaviour of TX1day events by M. Rothlisberger and L. Papritz (2023), we propose a variance decomposition of temperature anomaly that quantifies the contributions of physical processes on variability. For $n$ temperature anomaly samples $\{ T' \}_{1:n}$ assuming that $T'_i = $ adv$_i +$ adiab$_i + $ diab$_i$, we have that:

\begin{equation}
\label{eq:2}
 \begin{split}
   V(\{ T' \}_{1:n})  & = V(\{ \text{adv} \}_{1:n} ) + V(\{ \text{adiab} \}_{1:n} ) + V(\{ \text{diab} \}_{1:n} ) + \\
   & \qquad Cov(\{ \text{adv} \}_{1:n} ,\{ \text{adiab} \}_{1:n}) + Cov(\{ \text{adv} \}_{1:n},\{ \text{diab} \}_{1:n}) + Cov(\{ \text{adiab} \}_{1:n},\{ \text{diab} \}_{1:n})
\end{split}
\end{equation}

Applying \ref{eq:2} to every location on Earth yields a direct and systematic approach to the analysis of global patterns of hot extreme variability in the presence of complex dependencies between physical processes. The linearity of the covariance operator and the fact that it is un-normalized, in comparison to correlation for instance, allows for a more intuitive comparison of second-moment contributions. 

## Principle Component Analysis

Principle Component Analysis (PCA), known as Empirical Orthogonal Functions in climate sciences, is a linear transformation introduced by K. Pearson (1901) and first applied in climate sciences for weather prediction by E. Lorenz (1956). Given multivariate samples of dimension $p$, it expresses data into $p$ orthogonal linear combinations of the original coordinates, determined iteratively to maximize explained variance. It is commonly used as a dimension reduction method by truncating to coordinates with high explained variance and in machine learning to yield data with uncorrelated features.

We employ PCA to investigate the dependence structure of the temperature anomaly decomposition. Similar to the variance decomposition, the transform is applied independently to series of yearly maxima TX1day events at every location on Earth with the aim of providing insights in spatial variability. 

## Baselines for trajectory data

As was earlier discussed, trajectory-based analyses are often carried out case-by-case and thus difficult to systematically apply for a global study. We propose leveraging the flexibility of machine learning models to investigate trajectories of yearly maxima TX1day decompositions. To compare the predictive performance of the state-of-the-art model introduced in the next section, X standard baseline models are employed: constant final-value, a Long Short-Term Memory (LSTM) network and a Gated Recurrent Unit (GRU) network. 

LSTMs and GRUs are types of Recurrent Neural Networks (RNN) using feedback loops to allow outputs of one cell to be the inputs of a subsequent cell. LSTMs were introduced by Hochreiter and Schmidhuber (1997) to solve vanishing gradients in applications with long-term dependencies. Each LSTM layer is composed of three gates - the input, forget and output gates - that are recurrently computed one timestep at a time to update temporal relationships, expressed in current and hidden states. This complex structure implies considerable model flexibility, but is prone to over-fitting. GRUs are introduced by ... to reduce the number of model parameters by utilizing only two gates per recurrence. Figure X illustrates the architecture and the relevant equations. 

These models are applied to timeseries obtained by averaging back-trajectories integrated from genesis $t_g$ to times $t_{g+1}$,  $t_{g+2}$, ...,  $t_{0}$, where the final time is the last timestep before the day of the hot extreme. RNN architectures learn temporal patterns by ... and we therefore augment the data by applying a sliding window to each timeseries. 

Talk about cross-validation technique or parameter selection. 

Talk about used loss function

Talk about validation: we want it to be good at predicting 4 steps forward, but especially the final timesteps. Maybe a weighted loss? Could use this: 
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7038523/

## Shapelet-based neural networks

<!--chapter:end:rmd/02-methods.Rmd-->

# Data

We apply the aforementioned methodologies to ERA5 data with $0.5^{\circ} \times 0.5^{\circ}$ horizontal resolution, 137 vertical coordinates expressed as model-levels and 3-hour temporal resolution from 1980 to 2020. The coarser resolution from the full ERA5 product is required as variables such as ... are reported only in the chosen resolution. In addition, the computational costs of running multiple LAGRANTO back-trajectories per grid cell per year are limiting.  

<!--chapter:end:rmd/03-data.Rmd-->

# Quick start

Although this section might not be the latest version, the general idea won't
change. Please see 
[R bookdownplus Textbook](https://bookdown.org/baydap/bookdownplus) to keep up 
with the update.

## Preparation

Before starting, you have to install R, RStudio, bookdown package, and
other software and packages (i.e. Pandoc, LaTeX, rmarkdown, rticle,
knitr, etc.) which bookdown depends on. See the official 
[manual](https://bookdown.org/yihui/bookdown/) of bookdown for details. 
Additionally, if you want to produce a poster, phython must be installed before 
using, and the path of phython might have to be added to the environmental 
variables for Windows users.

## Installation

```
install.package("bookdownplus")
# or
devtools::
  install_github("pzhaonet/bookdownplus")
```

## Generate demo files

Run the following codes, and you will get some files (e.g. `index.Rmd`, 
`body.Rmd`, `bookdownplus.Rproj`) and folders in your working directory.

```
getwd() # this is your working directory. run setwd() to change it.
bookdownplus::bookdownplus()
```

## Build a demo book

Now open `bookdownplus.Rproj` with RStudio, and press `ctrl+shift+b` to compile
it. Your will get a book file named `*.pdf` in `_book/`folder.

## Write your own

Write your own text in `index.Rmd` and `body.Rmd`, and build your own lovely 
book.

## More outputs

By default, the book is in a pdf file. From 'bookdownplus' 1.0.3, users can get
more output formats, including 'word', 'html' and 'epub'. Run:

```
bookdownplus::
  bookdownplus(template = 'article',
               more_output = c('html', 'word', 'epub'))
```

## Recommendations

I have been developing some other packages, which bring more features into 
'bookdown', such as:

- mindr [@R-mindr], which can extract the outline of your book and turn it into
  a mind map, and

- pinyin [@R-pinyin], which can automatically generate ['{#ID}'](https://bookdown.org/yihui/bookdown/cross-references.html) of the 
  chapter headers even if there are Chinese characters in them.

Both of them have been released on CRAN and can be installed via:

```
install.packages('mindr')
install.packages('pinyin')
```

Enjoy your bookdowning!

## Models

Eq. \@ref(eq:mc2) is an equation.

\begin{equation}
E = mc^2
  (\#eq:mc2)
\end{equation}

It can be written as $E = mc^2$.

<!--chapter:end:rmd/03-quick-start.Rmd-->

# Results

## Relating TX1day T' variability to component variability

## Comparing contributions to TX1day T' mean and variance

## Inter-component variability structure

## Trajectory analysis with ML

<!--chapter:end:rmd/04-results.Rmd-->

# Conclusions

Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor
incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis
nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.
Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore
eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt
in culpa qui officia deserunt mollit anim id est laborum


<!--chapter:end:rmd/05-conclusion.Rmd-->

`r if (knitr::is_html_output()) '
# References {-}
'`

<!--chapter:end:rmd/99-references.Rmd-->

